{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82b2fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from networkx) (4.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.4.2-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 166.7 MB 13 kB/s /s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.19.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.5.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.4.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp36-cp36m-manylinux1_x86_64.whl (23.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.9 MB 18.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.5.3)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (0.8)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.1.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.0.1 smart-open-5.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 5.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-Levenshtein) (49.6.0.post20210108)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp36-cp36m-linux_x86_64.whl size=155944 sha256=7b0aa63f7a06a2efe5bf78b41e92632c69f085662b3a500fcf33dafe7d165d54\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4a/a4/bf/d761b0899395c75fa76d003d607b3869ee47f5035b8afc30a2\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install networkx\n",
    "!pip3 install xgboost\n",
    "!pip3 install gensim\n",
    "!pip install fuzzywuzzy\n",
    "!pip3 install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456fb72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630527c",
   "metadata": {},
   "source": [
    "## Graph features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4efda15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d022f7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 33225\n",
      "Number of edges: 532655\n"
     ]
    }
   ],
   "source": [
    "# Create a directed graph\n",
    "G = nx.read_weighted_edgelist('edgelist.txt', delimiter=' ', create_using=nx.DiGraph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b900ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training matrix. Each row corresponds to a pair of nodes and\n",
    "# its class label is 1 if it corresponds to an edge and 0, otherwise.\n",
    "# Use the following 4 features for each pair of nodes:\n",
    "# (1) in-degree of source node\n",
    "# (2) out-degree of source node\n",
    "# (3) in-degree of target node\n",
    "# (4) out-degree of target node\n",
    "X_train = np.zeros((2*m, 4))\n",
    "y_train = np.zeros(2*m)\n",
    "for i,edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    X_train[2*i,0] = G.in_degree(edge[0])\n",
    "    X_train[2*i,1] = G.out_degree(edge[0])\n",
    "    X_train[2*i,2] = G.in_degree(edge[1])\n",
    "    X_train[2*i,3] = G.out_degree(edge[1])\n",
    "    y_train[2*i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    X_train[2*i+1,0] = G.in_degree(n1)\n",
    "    X_train[2*i+1,1] = G.out_degree(n1)\n",
    "    X_train[2*i+1,2] = G.in_degree(n2)\n",
    "    X_train[2*i+1,3] = G.out_degree(n2)\n",
    "    y_train[2*i+1] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f5d5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))\n",
    "\n",
    "# Create the test matrix. Use the same 4 features as above\n",
    "X_test = np.zeros((len(node_pairs), 4))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = G.in_degree(node_pair[0])\n",
    "    X_test[i,1] = G.out_degree(node_pair[0])\n",
    "    X_test[i,2] = G.in_degree(node_pair[1])\n",
    "    X_test[i,3] = G.out_degree(node_pair[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f201b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b633e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3989e341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8421342144540087"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6434424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "y_pred = y_pred[:,1]\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce90b2bb",
   "metadata": {},
   "source": [
    "## Try some other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "499d614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5fb76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:36:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9114670846983507"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternative models\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "# Use XGBoost to predict if two nodes are linked by an edge\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d01b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "y_pred = y_pred[:,1]\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a546c5",
   "metadata": {},
   "source": [
    "## Text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a5ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b04d75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directed graph\n",
    "G = nx.read_weighted_edgelist('edgelist.txt', delimiter=' ', create_using=nx.DiGraph(), nodetype=int)\n",
    "m = G.number_of_edges()\n",
    "\n",
    "# Read the textual content of each domain name\n",
    "text = list()\n",
    "for i in range(33226):\n",
    "    with open('node_information/text/'+str(i)+'.txt', 'r', errors='ignore') as f:\n",
    "        text.append(f.read())\n",
    "\n",
    "# Map text to set of terms\n",
    "text = [set(text[i].split()) for i in range(len(text))]\n",
    "\n",
    "# Create the training matrix. Each row corresponds to a pair of nodes and\n",
    "# its class label is 1 if it corresponds to an edge and 0, otherwise.\n",
    "# Use the following 3 features for each pair of nodes:\n",
    "# (1) number of unique terms of the source node's textual content\n",
    "# (2) number of unique terms of the target node's textual content\n",
    "# (3) number of common terms between the textual content of the two nodes\n",
    "X_train = np.zeros((2*m, 3))\n",
    "y_train = np.zeros(2*m)\n",
    "n = G.number_of_nodes()\n",
    "for i,edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    X_train[2*i,0] = len(text[edge[0]])\n",
    "    X_train[2*i,1] = len(text[edge[1]])\n",
    "    X_train[2*i,2] = len(text[edge[0]].intersection(text[edge[1]]))\n",
    "    y_train[2*i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = randint(0, n-1)\n",
    "    n2 = randint(0, n-1)\n",
    "    X_train[2*i+1,0] = len(text[n1])\n",
    "    X_train[2*i+1,1] = len(text[n2])\n",
    "    X_train[2*i+1,2] = len(text[n1].intersection(text[n2]))\n",
    "    y_train[2*i+1] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2aad9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 33225\n",
      "Number of edges: 532655\n"
     ]
    }
   ],
   "source": [
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5459cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))\n",
    "\n",
    "# Create the test matrix. Use the same 4 features as above\n",
    "X_test = np.zeros((len(node_pairs), 3))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = len(text[node_pair[0]])\n",
    "    X_test[i,1] = len(text[node_pair[1]])\n",
    "    X_test[i,2] = len(text[node_pair[0]].intersection(text[node_pair[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd598562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6532915301649285"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5442125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "029f7737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:03:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7735034872478433"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternative models\n",
    "import xgboost as xgb\n",
    "# Use XGBoost to predict if two nodes are linked by an edge\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "y_pred = y_pred[:,1]\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c84dd",
   "metadata": {},
   "source": [
    "## combine graph & text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9febdf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 33225\n",
      "Number of edges: 532655\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.read_weighted_edgelist('edgelist.txt', delimiter=' ', create_using=nx.DiGraph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)\n",
    "\n",
    "# Read the textual content of each domain name\n",
    "text = list()\n",
    "for i in range(33226):\n",
    "    with open('node_information/text/'+str(i)+'.txt', 'r', errors='ignore') as f:\n",
    "        text.append(f.read())\n",
    "\n",
    "# Map text to set of terms\n",
    "text = [set(text[i].split()) for i in range(len(text))]\n",
    "\n",
    "#train\n",
    "X_train = np.zeros((2*m, 7))\n",
    "y_train = np.zeros(2*m)\n",
    "for i,edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    X_train[2*i,0] = G.in_degree(edge[0])\n",
    "    X_train[2*i,1] = G.out_degree(edge[0])\n",
    "    X_train[2*i,2] = G.in_degree(edge[1])\n",
    "    X_train[2*i,3] = G.out_degree(edge[1])\n",
    "    X_train[2*i,4] = len(text[edge[0]])\n",
    "    X_train[2*i,5] = len(text[edge[1]])\n",
    "    X_train[2*i,6] = len(text[edge[0]].intersection(text[edge[1]]))\n",
    "    y_train[2*i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    X_train[2*i+1,0] = G.in_degree(n1)\n",
    "    X_train[2*i+1,1] = G.out_degree(n1)\n",
    "    X_train[2*i+1,2] = G.in_degree(n2)\n",
    "    X_train[2*i+1,3] = G.out_degree(n2)\n",
    "    X_train[2*i+1,4] = len(text[n1])\n",
    "    X_train[2*i+1,5] = len(text[n2])\n",
    "    X_train[2*i+1,6] = len(text[n1].intersection(text[n2]))\n",
    "    y_train[2*i+1] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3cd2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))\n",
    "\n",
    "# Create the test matrix. Use the same 7 features as above\n",
    "X_test = np.zeros((len(node_pairs), 7))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = G.in_degree(node_pair[0])\n",
    "    X_test[i,1] = G.out_degree(node_pair[0])\n",
    "    X_test[i,2] = G.in_degree(node_pair[1])\n",
    "    X_test[i,3] = G.out_degree(node_pair[1])\n",
    "    X_test[i,4] = len(text[node_pair[0]])\n",
    "    X_test[i,5] = len(text[node_pair[1]])\n",
    "    X_test[i,6] = len(text[node_pair[0]].intersection(text[node_pair[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19b9cc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8751781171677727"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caebd549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.4.2-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 166.7 MB 12 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.5.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.19.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.4.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdff7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:08:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9301339516197163"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternative models\n",
    "import xgboost as xgb\n",
    "# Use XGBoost to predict if two nodes are linked by an edge\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f449bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "y_pred = y_pred[:,1]\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4263cce",
   "metadata": {},
   "source": [
    "## Combined text vectorization doc2vec + Graph + Deep Walk + Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf19ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Combined Graph, deepwalk & text\n",
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from random import choice\n",
    "from deepwalk import deepwalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ea576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 33225\n",
      "Number of edges: 506748\n"
     ]
    }
   ],
   "source": [
    "#use undirected graph (see below)\n",
    "#create an undirected graph G_ for the deepwalk embeddings\n",
    "G_ = nx.read_edgelist('edgelist.txt', delimiter=' ', create_using=nx.Graph(), nodetype=int)\n",
    "nodes_ = list(G_.nodes())\n",
    "edges_ = list(G_.edges())\n",
    "n_ = G_.number_of_nodes()\n",
    "m_ = G_.number_of_edges()\n",
    "print('Number of nodes:', n_)\n",
    "print('Number of edges:', m_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98174919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 33225\n",
      "Number of edges: 532655\n"
     ]
    }
   ],
   "source": [
    "# Create a directed graph for in_degrees & out_degrees\n",
    "#G = nx.read_edgelist('edgelist.txt', delimiter=' ', create_using=nx.Graph(), nodetype=int)\n",
    "G = nx.read_weighted_edgelist('edgelist.txt', delimiter=' ', create_using=nx.DiGraph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "edges = list(G.edges())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408ab277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text of each website\n",
    "text = list()\n",
    "node_list = list()\n",
    "for i in range(33226):\n",
    "     with open('node_information/text/'+str(i)+'.txt', 'r', errors='ignore') as f:\n",
    "        text.append(f.read())\n",
    "        node_list.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e926e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the nodes and the texts into a dictionary for embedding\n",
    "text_dict = dict(zip(node_list, text))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "941cbd36",
   "metadata": {},
   "source": [
    "for node in text_dict:\n",
    "    text_dict[node] = text_dict[node].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68633134",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [node]) for node, doc in text_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2422f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents, vector_size=32, window=10, dm=0, hs=1, min_count=20, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19a83a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:4: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n"
     ]
    }
   ],
   "source": [
    "# You can obtain the embedding of the abstact of a node using: model.docvecs[node]\n",
    "E = np.zeros((n_+1, 32))\n",
    "for node in nodes:\n",
    "    E[node,:] = model.docvecs[node]\n",
    "\n",
    "np.save('doc_vectors', E)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c526c743",
   "metadata": {},
   "source": [
    "# Map text to set of terms\n",
    "for node in text_dict:\n",
    "    text_dict[node] = set(text_dict[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fdb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the textual content of each domain name\n",
    "text = list()\n",
    "for i in range(33226):\n",
    "    with open('node_information/text/'+str(i)+'.txt', 'r', errors='ignore') as f:\n",
    "        text.append(f.read())\n",
    "\n",
    "# Map text to set of terms\n",
    "text = [set(text[i].split()) for i in range(len(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0e859bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deepwalk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-281814e15b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_walks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwalk_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_walks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deepwalk' is not defined"
     ]
    }
   ],
   "source": [
    "#generate random walks on the undirected graph\n",
    "# Extracts a set of random walks from the network and feeds them to the Skipgram model\n",
    "n_dim = 64\n",
    "n_walks = 5\n",
    "walk_length = 10\n",
    "model = deepwalk(G_, n_walks, walk_length, n_dim) \n",
    "\n",
    "embeddings = np.zeros((n+1, n_dim))\n",
    "for node_ in G_.nodes():\n",
    "    embeddings[node_,:] = model.wv[str(node_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40570af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the textual content of each domain name\n",
    "text = list()\n",
    "for i in range(33226):\n",
    "    with open('node_information/text/'+str(i)+'.txt', 'r', errors='ignore') as f:\n",
    "        text.append(f.read())\n",
    "\n",
    "# Map text to set of terms\n",
    "text = [set(text[i].split()) for i in range(len(text))]\n",
    "\n",
    "#train\n",
    "X_train = np.zeros((2*m, 15))\n",
    "y_train = np.zeros(2*m)\n",
    "for i,edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    X_train[2*i,0] = G.in_degree(edge[0]) #in degree source node\n",
    "    X_train[2*i,1] = G.out_degree(edge[0]) #out degree esource node\n",
    "    X_train[2*i,2] = G.in_degree(edge[1]) #in degree target node\n",
    "    X_train[2*i,3] = G.out_degree(edge[1]) #out degree target node \n",
    "    X_train[2*i,4] = len(text[edge[0]]) #number of unique terms of the source node's textual content\n",
    "    X_train[2*i,5] = len(text[edge[1]]) #number of unique terms of the target node's textual content\n",
    "    X_train[2*i,6] = len(text[edge[0]].intersection(text[edge[1]])) #number of common terms between the textual content of the two nodes\n",
    "    X_train[2*i,7] = G_.degree(edge[0]) + G_.degree(edge[1]) #sum of degrees of two nodes\n",
    "    X_train[2*i,8] = abs(G_.degree(edge[0]) - G_.degree(edge[1])) #abs diff of degrees of two nodes \n",
    "    X_train[2*i,9] = np.dot(embeddings[edge[0],:], embeddings[edge[1],:])/(np.linalg.norm(embeddings[edge[0],:])*np.linalg.norm(embeddings[edge[1],:])) #cosine similarity of embeddings of two nodes\n",
    "    X_train[2*i,10] = len(text_dict[edge[0]]) + len(text_dict[edge[1]]) #sum unique terms of two nodes of pages\n",
    "    X_train[2*i,11] = abs(len(text_dict[edge[0]]) - len(text_dict[edge[1]])) #abs val of diff of nb unique terms of the two nodes' pages\n",
    "    X_train[2*i,12] = len(text_dict[edge[0]].intersection(text_dict[edge[1]])) #nb of common terms betweeen pages of the two nodes\n",
    "    X_train[2*i,13] = np.dot(E[edge[0],:], E[edge[1],:])/(np.linalg.norm(E[edge[0],:])*np.linalg.norm(E[edge[1],:])) #cosine similarity of embeddings of pages\n",
    "    X_train[2*i,14] = np.linalg.norm(E[edge[0],:]-E[edge[1],:])    #euclidian distance of embeddings of pages\n",
    "    y_train[2*i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    X_train[2*i+1,0] = G.in_degree(n1) #in degree source node\n",
    "    X_train[2*i+1,1] = G.out_degree(n1)  #out degree esource node\n",
    "    X_train[2*i+1,2] = G.in_degree(n2) #in degree target node\n",
    "    X_train[2*i+1,3] = G.out_degree(n2) #out degree target nod\n",
    "    X_train[2*i+1,4] = len(text[n1])  #number of unique terms of the source node's textual content\n",
    "    X_train[2*i+1,5] = len(text[n2]) #number of unique terms of the target node's textual content\n",
    "    X_train[2*i+1,6] = len(text[n1].intersection(text[n2])) #number of common terms between the textual content of the two nodes\n",
    "    X_train[2*i+1,7] = G_.degree(n1) + G_.degree(n2)  #sum of degrees of two nodes\n",
    "    X_train[2*i+1,8] = abs(G_.degree(n1) - G_.degree(n2)) #abs diff of degrees of two nodes \n",
    "    X_train[2*i+1,9] = np.dot(embeddings[n1,:], embeddings[n2,:])/(np.linalg.norm(embeddings[n1,:])*np.linalg.norm(embeddings[n2,:])) #cosine similarity of embeddings of two nodes\n",
    "    X_train[2*i+1,10] = len(text_dict[n1]) + len(text_dict[n2]) #sum unique terms of two nodes of pages\n",
    "    X_train[2*i+1,11] = abs(len(text_dict[n1]) - len(text_dict[n2])) #abs val of diff of nb unique terms of the two nodes' pages\n",
    "    X_train[2*i+1,12] = len(text_dict[n1].intersection(text_dict[n2])) #nb of common terms betweeen pages of the two nodes\n",
    "    X_train[2*i+1,13] = np.dot(E[n1,:], E[n2,:])/(np.linalg.norm(E[n1,:])*np.linalg.norm(E[n2,:])) #cosine similarity of embeddings of pages\n",
    "    X_train[2*i+1,14] = np.linalg.norm(E[n1,:]-E[n2,:]) #euclidian distance of embeddings of pages\n",
    "    y_train[2*i+1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23aa816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))\n",
    "\n",
    "# Create the test matrix. Use the same 15 features as above\n",
    "X_test = np.zeros((len(node_pairs), 15))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = G.in_degree(node_pair[0])\n",
    "    X_test[i,1] = G.out_degree(node_pair[0])\n",
    "    X_test[i,2] = G.in_degree(node_pair[1])\n",
    "    X_test[i,3] = G.out_degree(node_pair[1])\n",
    "    X_test[i,4] = len(text[node_pair[0]])\n",
    "    X_test[i,5] = len(text[node_pair[1]])\n",
    "    X_test[i,6] = len(text[node_pair[0]].intersection(text[node_pair[1]]))\n",
    "    X_test[i,7] = G_.degree(node_pair[0]) + G_.degree(node_pair[1])\n",
    "    X_test[i,8] = abs(G_.degree(node_pair[0]) - G_.degree(node_pair[1]))\n",
    "    X_test[i,9] = np.dot(embeddings[node_pair[0],:], embeddings[node_pair[1],:])/(np.linalg.norm(embeddings[node_pair[0],:])*np.linalg.norm(embeddings[node_pair[1],:]))\n",
    "    X_test[i,10] = len(text_dict[node_pair[0]]) + len(text_dict[node_pair[1]])\n",
    "    X_test[i,11] = abs(len(text_dict[node_pair[0]]) - len(text_dict[node_pair[1]]))\n",
    "    X_test[i,12] = len(text_dict[node_pair[0]].intersection(text_dict[node_pair[1]]))\n",
    "    X_test[i,13] = np.dot(E[node_pair[0],:], E[node_pair[1],:])/(np.linalg.norm(E[node_pair[0],:])*np.linalg.norm(E[node_pair[1],:]))\n",
    "    X_test[i,14] = np.linalg.norm(E[node_pair[0],:]-E[node_pair[1],:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fd1d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative models\n",
    "import xgboost as xgb\n",
    "# Use XGBoost to predict if two nodes are linked by an edge\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fbe6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "y_pred = y_pred[:,1]\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef1992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "95c1f680",
   "metadata": {},
   "source": [
    "# Create the training matrix. Each row corresponds to a pair of nodes and\n",
    "# its class label is 1 if it corresponds to an edge and 0, otherwise.\n",
    "# Use the following 5 features for each pair of nodes:\n",
    "# (1) sum of number of unique terms of the two nodes' abstracts\n",
    "# (2) absolute value of difference of number of unique terms of the two nodes' abstracts\n",
    "# (3) number of common terms between the abstracts of the two nodes\n",
    "# (4) cosine similarity of embeddings of abstracts\n",
    "# (5) euclidean distance of embeddings of abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1686da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "577ba9e2",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98efb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords removal french "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d66bd10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccd0ca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 104]\n",
      "[nltk_data]     Connection reset by peer>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c78097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in '.../corpora/stopwords' (not loaded yet)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = stopwords.words('french')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29b14e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bfd72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stopwords = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b2dd4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list()\n",
    "pages = dict()\n",
    "list_pages = list()\n",
    "punctuation = set(string.punctuation)\n",
    "counter = 0\n",
    "for i in range(33226):\n",
    "    with open('node_information/text/'+str(i)+'.txt', 'r', errors='ignore') as f:\n",
    "        text.append(f.read())\n",
    "\n",
    "        for idx, line in enumerate(text):\n",
    "        node, page = idx, line\n",
    "        processed_abstract = ''.join([w if w not in punctuation else ' ' for w in abstract]) #.lower()\n",
    "    abstract = [word for word in processed_abstract.split() if word not in stopwords]\n",
    "    abstracts[int(node)] = abstract\n",
    "    list_abstracts.append(abstract)\n",
    "    counter += 1\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Read the abstract of each paper\n",
    "abstracts = dict()\n",
    "list_abstracts = list()\n",
    "punctuation = set(string.punctuation)\n",
    "counter = 0\n",
    "for idx, line in enumerate(text):\n",
    "    node, abstract = idx, line\n",
    "    processed_abstract = ''.join([w if w not in punctuation else ' ' for w in abstract]) #.lower()\n",
    "    abstract = [word for word in processed_abstract.split() if word not in stopwords]\n",
    "    abstracts[int(node)] = abstract\n",
    "    list_abstracts.append(abstract)\n",
    "    counter += 1\n",
    "\n",
    "# Map text to set of terms\n",
    "for node in abstracts:\n",
    "    abstracts[node] = set(abstracts[node])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42910767",
   "metadata": {},
   "source": [
    "## Code from previous challenge"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b6f18d6",
   "metadata": {},
   "source": [
    "text = list()\n",
    "for i in range(33226):\n",
    "    with open('node_information/text/'+str(i)+'.txt', 'r', errors='ignore') as f:\n",
    "        text.append(f.read())\n",
    "\n",
    "# Map text to set of terms\n",
    "text = [set(text[i].split()) for i in range(len(text))]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a51748b0",
   "metadata": {},
   "source": [
    "# TEXT\n",
    "\n",
    "# Read the abstract of each paper\n",
    "abstracts = dict()\n",
    "list_abstracts = list()\n",
    "punctuation = set(string.punctuation)\n",
    "counter = 0\n",
    "\n",
    "with open('abstracts.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        node, abstract = line.split('|--|')\n",
    "        processed_abstract = ''.join([w if w not in punctuation else ' ' for w in abstract.lower()])\n",
    "        abstract = [word for word in processed_abstract.split() if word not in stopwords]\n",
    "        abstracts[int(node)] = abstract\n",
    "        list_abstracts.append(abstract)\n",
    "        counter += 1\n",
    "\n",
    "# Map text to set of terms\n",
    "for node in abstracts:\n",
    "    abstracts[node] = set(abstracts[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb0ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the abstract of each paper\n",
    "abstracts = dict()\n",
    "list_abstracts = list()\n",
    "punctuation = set(string.punctuation)\n",
    "counter = 0\n",
    "for idx, line in enumerate(text):\n",
    "    node, abstract = idx, line\n",
    "    processed_abstract = ''.join([w if w not in punctuation else ' ' for w in abstract]) #.lower()\n",
    "    abstract = [word for word in processed_abstract.split() if word not in stopwords]\n",
    "    abstracts[int(node)] = abstract\n",
    "    list_abstracts.append(abstract)\n",
    "    counter += 1\n",
    "\n",
    "# Map text to set of terms\n",
    "for node in abstracts:\n",
    "    abstracts[node] = set(abstracts[node])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6462a90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33226 33226 33225\n"
     ]
    }
   ],
   "source": [
    "print(len(abstracts), counter, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae3cbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4ea77ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# WORD2VEC\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_w2v = Word2Vec(vector_size=64, window=5, min_count=0, sg=1, workers=8)\n",
    "model_w2v.build_vocab(list_abstracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73ea4b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1274833, 1280935)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.train(list_abstracts, total_examples=model_w2v.corpus_count, epochs=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c0bfe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 64\n",
    "coutner =0\n",
    "embeddings_abstracts = np.zeros((n, n_dim))\n",
    "pass_nodes = list()\n",
    "for node in nodes:\n",
    "  if len(abstracts[int(node)]) > 1:\n",
    "    embeddings_abstracts[node,:] = model_w2v.wv[abstracts[int(node)]].mean(0)\n",
    "  else:\n",
    "    pass_nodes.append(node)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "873a7844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20421, 14319, 6807, 3409, 8019, 6108, 29573, 3753, 2997, 3855, 6799, 19735, 8523, 501, 22055, 26112, 5381, 4545, 3959, 1634, 5437, 5537, 21641, 5540, 18300, 5562, 5548, 5530, 5542, 5547, 5538, 5554, 5543, 6277, 347, 341, 6033, 402, 5944, 8531, 7659, 4829, 10715, 19512, 8640, 8082, 6055, 5622, 3841, 20288, 3304, 20287, 20294, 6813, 18672, 14734, 5620, 15752, 17443, 15494, 4927, 3764, 20271, 85, 9985, 6734, 20309, 20268, 19931, 213, 20282, 14667, 10232, 3079, 3134, 6397, 9071, 1770, 8118, 873, 639, 6567, 13039, 5447, 9830, 5461, 4528, 9912, 17593, 6048, 225, 343, 11480, 3567, 7282, 8964, 5034, 908, 11310, 7300, 503, 8405, 33119, 1867, 30078, 8716, 1866, 6708, 5761, 9461, 11802, 3444, 1873, 20250, 17186, 13992, 26923, 20796, 9773, 3448, 6487, 2888, 6616, 5067, 9005, 5843, 14061, 23603, 25701, 19579, 6107, 5929, 19706, 1868, 18326, 9965, 4458, 20164, 12003, 6433, 14614, 12294, 11431, 13203, 14645, 602, 18158, 25558, 4947, 18149, 13111, 14009, 5091, 13325, 1077, 11060, 13434, 14300, 31730, 14672, 11408, 5047, 8392, 8472, 13309, 11081, 9885, 7012, 18245, 22506, 28398, 11486, 14620, 13340, 8147, 30804, 23587, 3453, 18250, 13333, 18170, 22633, 8779, 21122, 4820, 1032, 5698, 19828, 22848, 5359, 25072, 22535, 5506, 6437, 16166, 5006, 16997, 7909, 23351, 22803, 7273, 20006, 10637, 16132, 19965, 31105, 18307, 4863, 32555, 25073, 5602, 1922, 28252, 19800, 16347, 16121, 11944, 4117, 5682, 16112, 30230, 5583, 4240, 15474, 16147, 8922, 3281, 6074, 8526, 28963, 32421, 280, 4351, 23122, 24253, 16631, 515, 6603, 230, 9756, 12580, 3682, 20407, 19274, 4549, 32375, 13802, 24621, 31626, 1112, 435, 14301, 16340, 29664, 9413, 9403, 10775, 9407, 9423, 9411, 23264, 20971, 9391, 9395, 10773, 7055, 31138, 8747, 30066, 11101, 540, 24114, 21727, 2983, 32765, 13259, 3340, 9174, 9176, 11268, 6480, 16076, 13118, 17330, 1664, 6660, 11659, 1569, 17431, 18203, 11466, 443, 5052, 23187, 12067, 499, 32794, 7530, 19662, 31996, 7529, 22710, 24702, 2867, 337, 12622, 12627, 12629, 3927, 12624, 4072, 13301, 31697, 25119, 11889, 21262, 22461, 20872, 1870, 16279, 7863, 14544, 5848, 10527, 8443, 5683, 31736, 20774, 23837, 3286, 22792, 31956, 23643, 31259, 4482, 95, 3324, 22636, 6758, 10268, 14285, 3785, 4253, 8364, 11274, 3861, 7571, 7365, 7364, 6429, 11287, 9635, 11481, 1718, 1714, 24800, 24532, 9205, 11997, 32314, 21001, 32293, 32174, 24972, 16708, 11273, 9331, 32307, 32302, 30292, 9329, 32300, 2526, 31484, 20385, 13737, 8534, 12331, 21043, 12332, 21042, 12333, 7382, 2237, 19661, 6643, 502, 20141, 2164, 16382, 16004, 15974, 15968, 15986, 15989, 15990, 15927, 15966, 15937, 15977, 15965, 15956, 15948, 15929, 15924, 15922, 15938, 16003, 15967, 1782, 15934, 29985, 27150, 13190, 27980, 3929, 18733, 5338, 10866, 2870, 19923, 21320, 21611, 17626, 28730, 3868, 3349, 27589, 6097, 17668, 8543, 17620, 370, 17008, 29193, 24284, 11542, 22944, 1464, 19526, 6406, 30360, 28200, 2917, 2906, 2860, 11827, 3254, 11817, 24148, 32514, 18196, 10108, 5846, 28225, 23209, 5150, 677, 33124, 31733, 4722, 12091, 23031, 9775, 23017, 23036, 23054, 1761, 11129, 9798, 2261, 5017, 21569, 20855, 561, 4468, 3055, 22983, 12117, 19408, 10091, 16837, 11586, 10518, 19420, 3047, 4435, 23016, 23050, 8206, 28722, 31749, 31828, 19472, 7353, 7662, 3263, 4629, 4631, 13413, 27713, 1879, 1878, 30821, 13470, 13766, 2901, 2903, 2902, 1773, 30137, 10988, 19506, 7541, 8229, 10974, 5051, 14479, 30300, 19363, 8471, 9852, 10831, 30472, 32679, 7363, 959, 16415, 7477, 29671, 28625, 24278, 84, 28427, 23256, 494, 9655, 20022, 22869, 22872, 8697, 1096, 14187, 15655, 652, 3451, 13891, 24558, 7479, 23135, 12753, 12024, 23196, 11369, 5131, 28420, 22768, 415, 8074, 28297, 442, 237, 13884, 7523, 19542, 17899, 24570, 19371, 25962, 13365, 26373, 19696, 19721, 1604, 1602, 1592, 18214, 18174, 29402, 29013, 12321, 16426, 14032, 15794, 28300, 20586, 15950, 16002, 15949, 15928, 25, 23293, 17622, 31906, 29248, 9863, 9859, 30553, 28246, 32374, 31467, 11503, 10929, 661, 11694, 32619, 23102, 10052, 30102, 13989, 6307, 19728, 19956, 32235, 3963, 23117, 28214, 5049, 28072, 20432, 4214, 16959, 13725, 3082, 4843, 4954, 5004, 4974, 4814, 4939, 4924, 4122, 4864, 4812, 1914, 4900, 4962, 4972, 4966, 4911, 16612, 19740, 32722, 10407, 9758, 12429, 19904, 22531, 9904, 23827, 3969, 763, 24102, 4430, 14443, 5428, 11537, 12081, 13952, 17920, 17230, 251, 7578, 5119, 5519, 13449, 9104, 12540, 6463, 14281, 8329, 5154, 23182, 11208, 11404, 14074, 5148, 8152, 3755, 18223, 29252, 11628, 8439, 8486, 23202, 1078, 1075, 18854, 32711, 20474, 7878, 28577, 31734, 372, 32142, 18152, 15749, 18252, 15580, 11463, 858, 20666, 8601, 13099, 5375, 7821, 1674, 10104, 22964, 19395, 32637, 7767, 8981, 1055, 5604, 7566, 12856, 28743, 31941, 18959, 20969, 7860, 13368, 23167, 17536, 23169, 23168, 9890, 14131, 4893, 20425, 6748, 16012, 471, 20863, 663, 31980, 4630, 3336, 13811, 13853, 28390, 25249, 23780, 15801, 10717, 3873, 31848, 1279, 20389, 24527, 11838, 22744, 12127, 32850, 31629, 8755, 318, 18791, 3973, 14688, 14670, 17086, 33079, 9584, 15462, 17374, 233, 6013, 7266, 32927, 29530, 29533, 29535, 13186, 11418, 10523, 32175, 24588, 12103, 8695, 11512, 8569, 32615, 32498, 19374, 5100, 21631, 32009, 20337, 32906, 32905, 1583, 25793, 10972, 24506, 21436, 747, 736, 17381, 26887, 10778, 4976, 14024, 1073, 32793, 2184, 12910, 22590, 22389, 32563, 13925, 12914, 11018, 22938, 19257, 13049, 1041, 14185, 13406, 13138, 6295, 2019, 1104, 10059, 1070, 1045, 1044, 1120, 1107, 1110, 1118, 1099, 1115, 1043, 20794, 1101, 1087, 29788, 7550, 23423, 4346, 8194, 15899, 14256, 13980, 28163, 25568, 30229, 17989, 17984, 29835, 22908, 13101, 7832, 8109, 13885, 8908, 19323, 28709, 21033, 11280, 252, 20812, 29770, 21002, 1408, 8730, 14134, 6724, 17024, 6714, 13903, 17663, 21906, 13337, 5045, 13324, 22714, 29063, 12233, 483, 2934, 468, 3952, 5094, 3968, 52, 22685, 5603, 19848, 19249, 1882, 28039, 32355, 17925, 7631, 18315, 8959, 22674, 24356, 22929, 20332, 20702, 14692, 21046, 18154, 24189, 3943, 3216, 3198, 3213, 3201, 3199, 3210, 3208, 26692, 5433, 28850, 4309, 11329, 16303, 13251, 16423, 6577, 917, 14204, 23219, 22840, 25179, 18494, 18514, 18498, 18459, 27934, 19094, 32941, 11972, 6730, 32233, 13460, 25946, 6294, 6727, 25038, 9928, 23357, 29750, 3329, 13295, 3285, 12591, 29255, 16964, 21171, 6644, 20927, 1309, 7904, 691, 11644, 14012, 16968, 3604, 6646, 8987, 9703, 16105, 12126, 13076, 20500, 6893, 8135, 5563, 21028, 472, 16167, 2733, 18606, 10690, 21781, 14293, 13829, 13827, 13846, 20988, 3124, 19513, 2128, 10063, 2118, 2107, 13730, 9787, 11127, 9970, 13494, 17855, 25131, 23042, 28065, 1501, 23588, 6309, 18669, 9098, 29741, 14697, 1854, 12595, 11477, 13747, 27570, 4581, 20150, 12300, 13984, 5959, 10819, 7839, 24299, 6103, 1051, 13009, 20802, 23598, 31685, 6417, 13739, 13347, 584, 13892, 7584, 28230, 9947, 2815, 11015, 23334, 22711, 26693, 30215, 5379, 16028, 15518, 32849, 11379, 31229, 4042, 24321, 31983, 17185, 4412, 4406, 4380, 15829, 28206, 11515, 24460, 5813, 5812, 5097, 5828, 5816, 3798, 26796, 11990, 11290, 5598, 10697, 16300, 21243, 30057, 27363, 18259, 19707, 480, 23773, 14481, 10117, 13849, 28561, 25100, 7914, 28224, 10793, 11216, 11593, 8070, 6113, 30126, 9304, 17873, 19325, 8682, 19361, 28438, 9953, 16199, 22663, 26609, 9764, 28483, 26260, 19428, 14677, 33003, 21199, 17353, 1996, 2174, 19320, 1138, 28257, 15666, 27156, 15843, 31812, 18218, 16916, 15716, 182, 13965, 5095, 6385, 16336, 6943, 5070, 16093, 3954, 5085, 23595, 28002, 24125, 18262, 8360, 14699, 8031, 5038, 15840, 5065, 22137, 6886, 1667, 391, 5066, 7487, 20667, 3125, 8487, 15658, 12577, 5139, 31904, 24602, 6444, 5156, 5987, 3092, 3009, 22479, 2026, 3081, 5009, 18460, 6641, 18504, 8280, 11148, 18493, 13792, 31708, 15696, 13272, 6281, 8656, 22735, 21248, 218, 23813, 5114, 7646, 11650, 11646, 3447, 11611, 11652, 11654, 11604, 11634, 6445, 11657, 6965, 11635, 11621, 11641, 5979, 4570, 11655, 11642, 5436, 17339, 23251, 24349, 20155, 18340, 32098, 31813, 5133, 29410, 26612, 7513, 16191, 8154, 12223, 5048, 17336, 18032, 29274, 12594, 31844, 32432, 10224, 29688, 29863, 11284, 397, 323, 346, 410, 449, 23717, 7164, 3364, 28951, 6446, 7974, 7390, 7389, 7405, 15865, 20212, 12463, 12516, 12477, 12484, 12532, 8671, 19950, 9334, 23824, 19465, 13475, 549, 20758, 28096, 16978, 4246, 22953, 3404, 4614, 17059, 29442, 28075, 5093, 16823, 793, 5897, 22618, 28501, 12985, 13879, 16660, 18491, 13673, 15751, 28633, 9288, 32673, 21720, 22962, 9905, 7270, 3490, 4556, 28038, 16401, 26521, 15844, 7575, 29475, 14073, 18051, 6482, 6735, 8837, 20808, 3179, 15582, 32458, 21304, 12019, 5891, 24500, 5055, 7352, 12292, 3831, 17039, 32213, 20057, 28194, 14290, 11011, 27978, 12063, 3194, 14408, 25097, 12846, 29621, 17213, 9314, 8978, 5555, 6413, 1069, 26286, 21281, 4184, 4175, 11579, 31387, 7497, 16406, 579, 29448, 7953, 14969, 29441, 14244, 9164, 28620, 74, 18008, 20746, 13657, 1435, 23331, 24669, 3380, 13187, 20800, 12607, 28323, 25197, 12106, 24586, 5356, 9418, 22578, 20144, 11017, 6458, 18136, 33091, 5459, 4669, 17889, 20843, 13990, 3084, 9223, 13289, 16661, 7285, 19726, 24235, 13175, 11527, 25615, 10779, 12025, 23201, 6296, 19015, 13011, 18436, 4582, 18091, 29121, 10720, 2024, 18831, 25479, 17433, 3569, 16603, 8105, 9989, 17442, 11098, 6873, 19222, 21182, 32564, 21940, 22645, 28107, 28103, 28123, 28129, 28130, 28115, 28132, 28118, 5954, 11826, 11820, 11825, 11831, 11821, 11835, 9742, 4724, 22556, 5423, 12544, 27987, 27988, 7818, 28548, 4469, 32305, 24651, 11214, 13322, 17926, 9982, 20243, 13933, 20326, 14038, 29961, 22517, 9973, 20158, 5132, 5772, 18816, 31961, 3342, 32571, 19366, 20283, 11447, 19417, 11602, 9636, 29157, 15022, 13919, 14010, 20043, 18838, 1829, 18153, 13488, 8706, 14405, 5072, 17808, 17848, 14578, 14572, 17847, 21568, 14563, 14556, 17837, 14590, 14573, 17833, 3395, 9290, 18594, 1747, 18781, 3769, 19577, 15547, 26761, 18605, 16258, 6191, 6958, 18258, 9468, 23239, 32442, 24203, 23271, 25788, 1416, 19821, 3766, 5128, 27355, 8502, 30168, 4501, 5370, 24728, 5071, 12755, 28084, 5731, 5374, 10955, 24502, 18325, 5099, 5081, 5089, 12758, 658, 23367, 28270, 5448, 17866, 28175, 5056, 14086, 166, 13262, 5041, 14279, 12018, 13158, 265, 4596, 24578, 20201, 7592, 5429, 19639, 29353, 13979, 32507, 11695, 13956, 2187, 12912, 32751, 5631, 2074, 26276, 21187, 21487, 31602, 16315, 8851, 4533, 2165, 12765, 16304, 28423, 23445, 17083, 5104, 3318, 22644, 1837, 7064, 32276, 21313, 11337, 9976, 23690, 20433, 33092, 5578, 5120, 26998, 17126, 16088, 21039, 3985, 21937, 15106, 13481, 1039, 20793, 19344, 24922, 17390, 248, 1079, 28570, 12880, 8089, 11487, 17974, 19754, 8084, 22749, 31163, 6756, 28742, 31840, 22747, 11560, 30657, 21706, 31180, 31054, 24609, 28088, 27693, 7778, 30516, 31847, 20024, 26422, 8136, 19256, 31000, 31845, 16789, 23292, 19917, 16405, 22796, 10835, 14286, 13290, 23193, 293, 7135, 14071, 17964, 19657, 13948, 13103, 1102, 28566, 9427, 3251, 15759, 29434, 32357, 24740, 5092, 18260, 31572, 32523, 23778, 33100, 2652, 27182, 7681, 11027, 17865, 5364, 16507, 9465, 30275, 10299, 23924, 16398, 19635, 11043, 18171, 20449, 22602, 23340, 15145, 27702, 28527, 15578, 15577, 1575, 22647, 5335, 23446, 26591, 25068, 23280, 6180, 5662, 17566, 19609, 23291, 5074, 7628, 24223, 18230, 30274, 17673, 8872, 20532, 11041, 30182, 6732, 22705, 6415, 6751, 32880, 19318, 20505, 17359, 18249, 25388, 6253, 21414, 5039, 29394, 17747, 31688, 11925, 16597, 2159, 28958, 20452, 24268, 14048, 27918, 8426, 26293, 9793, 275, 31190, 31461, 6990, 27791, 19663, 12098, 3546, 16061, 21579, 4261, 26395, 7810, 277, 24841, 18022, 5445, 12549, 10812, 16766, 4734, 7829, 24572, 19437, 8597, 6245, 6562, 31269, 11580, 20755, 14254, 3072, 3328, 3341, 3274, 3323, 14923, 17676, 16707, 1760, 1010, 31029, 15134, 11090, 6088, 7890, 9012, 12213, 20873, 6104, 21713, 7894, 20766, 13757, 17607, 29767, 8713, 29022, 8482, 11069, 19932, 5776, 17397, 2173, 15670, 19849, 16309, 6575, 26574, 16324, 22658, 26885, 27152, 22182, 16424, 22526, 30925, 14047, 6405, 13059, 24533, 18240, 14025, 16036, 22494, 21590, 12913, 18195, 6431, 23823, 9519, 19893, 28250, 12350, 32645, 30835, 22404, 7326, 13344, 864, 4108, 31909, 26946, 3491, 29499, 5453, 2058, 5642, 32994, 5713, 30055, 15568, 17064, 16841, 12576, 18192, 11345, 12327, 30910, 30107, 11417, 1092, 17606, 6304, 13134, 15615, 23533, 14034, 20249, 9333, 5893, 6072, 25587, 24075, 1119, 16651, 24559, 10798, 23454, 7269, 9877, 32543, 6145, 8760, 23284, 23364, 25611, 29192, 30072, 19784, 27795, 28442, 19302, 20772, 24825, 29140, 2078, 31309, 32878, 32534, 2971, 11111, 13889, 14638, 15877, 18942, 2414, 21592, 30641, 21334, 20182, 20192, 32419, 20180, 32045, 24504, 14918, 7290, 24318, 25777, 30774, 14050, 16394, 20487, 9823, 19416, 17695, 28256, 12020, 28263, 28261, 15099, 28254, 16323, 5123, 7588, 20631, 28410, 33113, 22971, 5625, 30070, 11803, 20588, 22474, 8823, 5791, 22418, 5468, 24509, 17906, 18272, 16400, 16358, 17003, 6384, 29804, 25151, 27664, 19531, 19543, 19530, 5444, 1525, 9971, 5112, 4017, 32182, 19215, 21970, 26002, 27533, 24047, 10864, 16073, 16503, 28497, 30802, 16035, 9032, 30050, 23433, 18596, 23254, 8029, 17809, 17344, 20130, 23636, 26138, 31820, 31325, 31816, 16197, 18830, 14313, 13320, 25547, 10385, 2810, 21309, 774, 27651, 25215, 7027, 13379, 30939, 18568, 22699, 28990, 14707, 30715, 7118, 29466, 25656, 21790, 12662, 20067, 17931, 4585, 12641, 29444, 20448, 13499, 31921, 22186, 29202, 12096, 24271, 24575, 4474, 8056, 5061, 5063, 29509, 18602, 5118, 5080, 29494, 12870, 5046, 5068, 5162, 5152, 5105, 5082, 18106, 5050, 5159, 23685, 5161, 23311, 30132, 19819, 1463, 20356, 6289, 24135, 8691, 13197, 17150, 5584, 13983, 4063, 16749, 18127, 25621, 21771, 11114, 11087, 9000, 14289, 5129, 31250, 32116, 28413, 16310, 22732, 10437, 28157, 27766, 30216, 32092, 17387, 29515, 29911, 21588, 10976, 17891, 26817, 25635, 32044, 26072, 25723, 4701, 2087, 24351, 27574, 19743, 28431, 15458, 12423, 6753, 27690, 8067, 16758, 17026, 31187, 12950, 18277, 4219, 11943, 28687, 20703, 18653, 6594, 23529, 12043, 31725, 17041, 17042, 4755, 21012, 10069, 23901, 12051, 24255, 10173, 23335, 25741, 19608, 1383, 21405, 20296, 20962, 20965, 3227, 20585, 31821, 14701, 23845, 21481, 24514, 23368, 29184, 12092, 20587, 26884, 15061, 31930, 5732, 11023, 20501, 17658, 7650, 13809, 4043, 31264, 7146, 15754, 30219, 6662, 9575, 11373, 9525, 31062, 4058, 278, 17395, 32845, 28521, 19166, 32705, 32846, 20209, 11430, 86, 3016, 6585, 3779, 12882, 30058, 19448, 17689, 19281, 29424, 25521, 3371, 9763, 28176, 24331, 16669, 15681, 21988, 1062, 19771, 20216, 26491, 27360, 21332, 4529, 6159, 17981, 14993, 24525, 30084, 5435, 30002, 20671, 13896, 26553, 10722, 11441, 29462, 23689, 20712, 32636, 12518, 12464, 12471, 12508, 12530, 12523, 12511, 12529, 12473, 12515, 12521, 10791, 28113, 18727, 15799, 31539, 25797, 10801, 22511, 22509, 22502, 12754, 20951, 29049, 9079, 21160, 17193, 25122, 32059, 30797, 32401, 22876, 5357, 5355, 5366, 5380, 5352, 19507, 21999, 23121, 24320, 30573, 10797, 31937, 6207, 9262, 23507, 7278, 29629, 17047, 31125, 24249, 16565, 14484, 4650, 26486, 24603, 16053, 6112, 11292, 32179, 3773, 5862, 8385, 5008, 27349, 18350, 20934, 6999, 17661, 16574, 16571, 11453, 10231, 24042, 24866, 13282, 11051, 15898, 11394, 18696, 10549, 4241, 25043, 20196, 20708, 20724, 20725, 23851, 29749, 20217, 12352, 26700, 23471, 7244, 27798, 30331, 15152, 9791, 16553, 18623, 31033, 13656, 32521, 28441, 23162, 26023, 19658, 16849, 5863, 2947, 4594, 21915, 21765, 589, 4028, 4050, 27974, 28860, 5496, 20383, 29017, 23829, 31638, 30080, 28479, 8438, 9310, 20828, 30067, 23314, 10588, 28476, 26430, 16335, 17802, 16925, 18852, 20570, 19633, 12973, 19464, 19476, 26063, 22416, 24787, 32851, 24465, 33087, 3180, 16853, 26464, 27010, 19890, 20167, 20333, 31652, 6163, 30409, 30609, 30404, 24099, 17870, 17966, 26787, 27894, 30678, 24324, 13899, 13803, 7425, 20891, 4730, 6944, 26025, 25266, 5737, 15662, 23687, 15103, 1185, 31745, 24968, 30349, 29810, 31554, 20521, 9327, 13098, 11184, 15117, 16872, 29311, 32974, 15575, 31540, 17350, 23573, 11485, 30424, 28965, 10785, 24738, 25023, 29131, 16797, 30383, 28970, 11003, 12527, 12524, 11397, 32803, 27959, 21399, 13452, 27002, 4005, 8430, 23146, 12798, 22563, 10079, 10603, 27272, 18957, 29766, 15739, 26504, 19634, 1462, 28675, 30554, 1539, 8607, 23478, 19297, 4670, 19220, 13163, 26478, 17282, 14276, 23586, 34, 10741, 32724, 24157, 31446, 22065, 13755, 5632, 25748, 26426, 30457, 23705, 30195, 22553, 24115, 30296, 32573, 12646, 232, 19630, 12737, 25524, 13363, 19556, 18573, 6171, 23718, 28899, 24736, 32856, 9430, 32146, 25705, 5106, 5101, 3848, 5087, 5144, 2814, 13157, 5469, 29461, 22628, 30543, 29972, 30816, 10766, 11801, 29428, 6972, 30938, 9780, 14739, 27516, 18995, 21467, 25742, 28216, 12901, 26246, 14033, 16146, 10991, 13865, 32592, 16730, 31021, 5455, 5490, 25171, 14292, 22149, 26559, 30779, 18276, 24464, 25761, 25346, 31384, 5567, 30869, 18006, 27347, 28471, 4441, 20636, 20637, 17538, 16528, 573, 22113, 18135, 4116, 22635, 32287, 25745, 16339, 16958, 20801, 28286, 32505, 32738, 12762, 2783, 6349, 9103, 13316, 31440, 20190, 3920, 26129, 14996, 29639, 16370, 5709, 31681, 22372, 12192, 28507, 30854, 11859, 21180, 9105, 32787, 32185, 32946, 6414, 27158, 8801, 7262, 1182, 12999, 24523, 81, 28557, 31526, 21255, 12903, 5664, 19876, 27120, 16735, 1980, 30864, 14390, 21979, 31094, 30419, 1744, 14646, 18998, 6100, 8562, 17603, 16605, 7426, 28983, 32525, 26682, 30176, 17937, 16754, 16164, 26788, 2114, 7976, 8674, 8885, 7830, 8075, 20825, 28977, 7664, 32449, 18176, 2109, 14094, 18201, 18361, 13882, 18843, 18254, 19871, 19767, 16245, 12669, 31582, 28412, 32867, 18209, 26910, 7437, 21578, 32750, 31947, 7537, 30043, 24806, 22878, 14312, 29807, 20358, 6581, 2605, 12852, 31332, 30115, 7501, 32763, 18009, 6536, 19833, 28301, 30785, 16417, 26233, 32013, 18077, 28146, 19666, 24783, 28982, 20657, 5098, 25071, 16342, 26773, 7449, 30801, 11124, 22913, 9209, 9207, 28538, 24599, 28405, 20252, 22243, 18661, 12877, 31175, 27178, 19612, 20788, 25165, 32963, 29982, 12828, 28991, 29698, 16145, 4443, 16561, 19863, 31706, 27956, 20548, 26028, 2653, 6501, 30972, 19886, 24123, 9311, 24130, 1804, 5601, 5621, 28879, 11149, 25251, 31395, 21279, 16656, 17737, 24790, 32328, 20569, 31628, 29924, 7379, 24055, 11504, 6857, 30188, 26432, 769, 5167, 31692, 19715, 24852, 21842, 22253, 32127, 11063, 15468, 25051, 29614, 29613, 31975, 21745, 28430, 28414, 26165, 28939, 25382, 30578, 19648, 21995, 22309, 4480, 19851, 20937, 14965, 20388, 24179, 6279, 13745, 30090, 28655, 31432, 25354, 25508, 27621, 13182, 24195, 12114, 24168, 24209, 24164, 24217, 25953, 31851, 30170, 11351, 19111, 19953, 31726, 19638, 29877, 9898, 19983, 16102, 19084, 32388, 16844, 4292, 14542, 15077, 22016, 4540, 18981, 9317, 27619, 23807, 26661, 6243, 29498, 24451, 25998, 31945, 16127, 14051, 31669, 2603, 28875, 27261, 30595, 24040, 22648, 19327, 30805, 32481, 14498, 24073, 32334, 22646, 29865, 20291, 28472, 30798, 30792, 32391, 30491, 26443, 21386, 30003, 3244, 31132, 18986, 12983, 20780, 7645, 6169, 8177, 26108, 2786, 2755, 29839, 11857, 31086, 29648, 13074, 10294, 23270, 18794, 32968, 24975, 28842, 30841, 10731, 20013, 20387, 30533, 26536, 27756, 12336, 13469, 2974, 16058, 20928, 32456, 27594, 29283, 26763, 25334, 13300, 12686, 22001, 26924, 19045, 32755, 29568, 10536, 30192, 13117, 22172, 27741, 27134, 27707, 23861, 19123, 12843, 6260, 13930, 26766, 27388, 13207, 18905, 22128, 31012, 25627, 30876, 23703, 28061, 15708, 24827, 14275, 25342, 24735, 32774, 16322, 9865, 20132, 14742, 20131, 20135, 31052, 7376, 5961, 18362, 17097, 14101, 24803, 6218, 16694, 29546, 22426, 14464, 26119, 22541, 10485, 9036, 28621, 5373, 22427, 32818, 5192, 19005, 33139, 19065, 18888, 28079, 17084, 33168, 32093, 2166, 29711, 27446, 19675, 23235, 17897, 16819, 28782, 28145, 29615, 25612, 31853, 2168, 19892, 29160, 22303, 30537, 26000, 13021, 19857, 24354, 20355, 33202, 31464, 28775, 32212, 7227, 32200, 28964, 17877, 19640, 31494, 24097, 24096, 27082, 10189, 19632, 21055, 6204, 28640, 32869, 2036, 26473, 32577, 32897, 31039, 21782, 20339, 32951, 26888, 31049, 20545, 23430, 32228, 5206, 25718, 3420, 26378, 19080, 32535, 27784, 22611, 3236, 32597, 33117, 27301, 26743, 26183, 6165, 19575, 24675, 23948, 25910, 21525, 27080, 5060, 24959, 10376, 26809, 27452, 22310, 20729, 30610, 26598, 21747, 30704, 31193, 25472, 28798, 28995, 27727, 27961, 30369, 20547, 28060, 21566, 27078, 16499, 32795, 24019, 32753, 26750, 30477, 23999, 12198, 14265, 11916, 29375, 32465, 25050, 17365, 31121, 26793, 30834, 10274, 31414, 24292, 30272, 3617, 29645, 31194, 2611, 18085, 21838, 30773, 23676, 13864, 25905, 13605, 32837, 31266, 14478, 27930, 22329, 16681, 30980, 30402, 29909, 29378, 20679, 21866, 4033, 4154, 29845, 27512, 28859, 12341, 10227, 10718, 23435, 16852, 9752, 14486, 27939, 13749, 32891, 23738, 14995, 23632, 18354, 31940, 32453, 25461, 26271, 31766, 24951, 31600, 14465, 32868, 32953, 22311, 26795, 27517, 30142, 29918, 27548, 28986, 26614, 26077, 22643, 30032, 31511, 21011, 23908, 18919, 28379, 30469, 30387, 25815, 31218, 30671, 23821, 4075, 26477, 4079, 4065, 33013, 26754, 29130, 26371, 4217, 15772, 26263, 7196, 31875, 32261, 28992, 14233, 30028, 27380, 25609, 11281, 24461, 28341, 25272, 29995, 27462, 33225, 27510, 26959, 10802, 29976, 14242, 31002, 16527, 20734, 26337, 19487, 25459, 27603, 4298, 15009, 27807, 25053, 32193, 697, 6305, 32103, 24919, 31870, 19866, 14239, 18357, 3977, 21884, 10993, 32745, 19952, 28984, 18358, 20031, 20516, 33186, 26206, 26389, 6178, 10508, 32843, 26034, 10314, 4070, 31486, 27032, 11283, 32908, 9942, 7153, 5740, 14483, 14495, 21808, 22052, 23438, 29117, 20059, 28189, 26147, 10218, 19637, 7493, 24060, 26369, 28320, 16791, 30924, 24317, 27348, 29587, 30006, 24893, 22331, 31375, 26940, 31639, 25264, 29994, 20589, 27460, 32940, 19682, 24323, 33122, 23427, 27252, 14994, 29595, 32155, 30763, 9941, 22270, 29827, 22071, 14512, 14273, 30019, 7238, 22314, 21849, 26404, 31319, 28579, 29898, 27124, 22151, 26862, 21969, 26570, 29445, 27616, 29094, 30209, 25579, 17149]\n"
     ]
    }
   ],
   "source": [
    "print(pass_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3661f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEPWALK\n",
    "############## Task 1\n",
    "# Simulates a random walk of length \"walk_length\" starting from node \"node\"\n",
    "def random_walk(G, node, walk_length):\n",
    "\twalk = [node]\n",
    "\t\n",
    "\tfor i in range(walk_length-1):\n",
    "\t\tnbrs = list(G.neighbors(walk[-1]))\n",
    "\t\tcurrent_node = choice(nbrs)\n",
    "\t\twalk.append(current_node)\n",
    "\n",
    "\twalk = [str(node) for node in walk]\n",
    "\treturn walk\n",
    "\n",
    "\n",
    "############## Task 2\n",
    "# Runs \"num_walks\" random walks from each node\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "\twalks = []\n",
    "\t\n",
    "\tnodes = list(G.nodes())\n",
    "\tfor i in range(num_walks):\n",
    "\t\tidx = np.random.permutation(len(nodes))\n",
    "\t\tfor j in range(len(nodes)):\n",
    "\t\t\tnode = nodes[idx[j]]\n",
    "\t\t\twalk = random_walk(G, node, walk_length)\n",
    "\t\t\twalks.append(walk)\n",
    "\n",
    "\treturn walks\n",
    "\n",
    "# Simulates walks and uses the Skipgram model to learn node representations\n",
    "def deepwalk(G, num_walks, walk_length, n_dim):\n",
    "    print(\"Generating walks\")\n",
    "    walks = generate_walks(G, num_walks, walk_length)\n",
    "\n",
    "    print(\"Training word2vec\")\n",
    "    model = Word2Vec(size=n_dim, window=8, min_count=0, sg=1, workers=8)\n",
    "    model.build_vocab(walks)\n",
    "    model.train(walks, total_examples=model.corpus_count, epochs=5)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a814eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 64\n",
    "n_walks = 5\n",
    "walk_length = 10\n",
    "model_dw = deepwalk(G, n_walks, walk_length, n_dim) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "11db2ad3",
   "metadata": {},
   "source": [
    "embeddings_walks = np.zeros((n, n_dim))\n",
    "for node in G.nodes():\n",
    "    embeddings_walks[node,:] = model_dw.wv[str(node)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a80620d",
   "metadata": {},
   "source": [
    "# AUTHORS\n",
    "authors = dict()\n",
    "list_authors = list()\n",
    "with open('authors.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        node, authors_names = line.split('|--|')\n",
    "        authors_names = authors_names.lower().strip()\n",
    "        authors[int(node)] = authors_names\n",
    "\n",
    "print(authors[0])\n",
    "\n",
    "for node in authors:\n",
    "    list_authors.append(authors[int(node)].split(','))\n",
    "\n",
    "authors_sets = list()\n",
    "for authors in list_authors:\n",
    "  authors = set(authors)\n",
    "  authors_sets.append(authors)\n",
    "\n",
    "print(list_authors[0])\n",
    "print(authors_sets[0], authors_sets[1])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e39fed2",
   "metadata": {},
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a8a8e7b",
   "metadata": {},
   "source": [
    "from fuzzywuzzy import fuzz,process\n",
    "\n",
    "print(len(list_authors))\n",
    "print(len(authors_sets[0].intersection(authors_sets[1])))\n",
    "Partial_Ratio = fuzz.partial_ratio(' '.join(list_authors[0]),' '.join(list_authors[1]))\n",
    "print(Partial_Ratio)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d5f535e",
   "metadata": {},
   "source": [
    "def generate_no_edge(nodes, n):\n",
    "    n1 = randint(0, n-1)\n",
    "    n2 = randint(0, n-1)\n",
    "    while G.has_edge(nodes[n1], nodes[n2]) == True:\n",
    "        n1 = randint(0, n-1)\n",
    "        n2 = randint(0, n-1)\n",
    "    return n1, n2\n",
    "\n",
    "X_train = np.zeros((2*m, 10))\n",
    "y_train = np.zeros(2*m)\n",
    "n = G.number_of_nodes()\n",
    "\n",
    "\n",
    "for i,edge in enumerate(G.edges()):\n",
    "\n",
    "\n",
    "    # an edge\n",
    "    X_train[2*i,0] = G.degree(edge[0]) + G.degree(edge[1])\n",
    "    X_train[2*i,1] = abs(G.degree(edge[0]) - G.degree(edge[1]))\n",
    "    X_train[2*i,2] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
    "    X_train[2*i,3] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
    "    X_train[2*i,4] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n",
    "    X_train[2*i,5] = np.dot(embeddings_walks[edge[0],:], embeddings_walks[edge[1],:])/(np.linalg.norm(embeddings_walks[edge[0],:])*np.linalg.norm(embeddings_walks[edge[1],:]))\n",
    "    X_train[2*i,6] = np.dot(embeddings_abstracts[edge[0],:], embeddings_abstracts[edge[1],:])/((np.linalg.norm(embeddings_abstracts[edge[0],:])*np.linalg.norm(embeddings_abstracts[edge[1],:])+1))\n",
    "    X_train[2*i,7] = len(authors_sets[edge[0]].intersection(authors_sets[edge[1]]))\n",
    "    X_train[2*i,8] = model_dw.wv.similarity(w1=str(edge[0]), w2=str(edge[1]))\n",
    "    X_train[2*i,9] = fuzz.partial_ratio(' '.join(list_authors[edge[0]]),' '.join(list_authors[edge[1]]))\n",
    "    y_train[2*i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "\n",
    "    n1, n2 = generate_no_edge(nodes, n)\n",
    "\n",
    "    X_train[2*i+1,0] = G.degree(n1) + G.degree(n2)\n",
    "    X_train[2*i+1,1] = abs(G.degree(n1) - G.degree(n2))\n",
    "    X_train[2*i+1,2] = len(abstracts[n1]) + len(abstracts[n2])\n",
    "    X_train[2*i+1,3] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
    "    X_train[2*i+1,4] = len(abstracts[n1].intersection(abstracts[n2]))\n",
    "    X_train[2*i+1,5] = np.dot(embeddings_walks[n1,:], embeddings_walks[n2,:])/(np.linalg.norm(embeddings_walks[n1,:])*np.linalg.norm(embeddings_walks[n2,:]))\n",
    "    X_train[2*i+1,6] = np.dot(embeddings_abstracts[n1,:], embeddings_abstracts[n2,:])/((np.linalg.norm(embeddings_abstracts[n1,:])*np.linalg.norm(embeddings_abstracts[n2,:])+1))\n",
    "    X_train[2*i+1,7] = len(authors_sets[n1].intersection(authors_sets[n2]))\n",
    "    X_train[2*i+1,8] = model_dw.wv.similarity(w1=str(n1), w2=str(n2))\n",
    "    X_train[2*i+1,9] = fuzz.partial_ratio(' '.join(list_authors[n1]),' '.join(list_authors[n2]))\n",
    "    \n",
    "    y_train[2*i+1] = 0\n",
    "\n",
    "print('Size of training matrix:', X_train.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec07ce23",
   "metadata": {},
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))\n",
    "\n",
    "# Create the test matrix. Use the same 4 features as above\n",
    "X_test = np.zeros((len(node_pairs), 10))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = G.degree(node_pair[0]) + G.degree(node_pair[1])\n",
    "    X_test[i,1] = abs(G.degree(node_pair[0]) - G.degree(node_pair[1]))\n",
    "    X_test[i,2] = len(abstracts[node_pair[0]]) + len(abstracts[node_pair[1]])\n",
    "    X_test[i,3] = abs(len(abstracts[node_pair[0]]) - len(abstracts[node_pair[1]]))\n",
    "    X_test[i,4] = len(abstracts[node_pair[0]].intersection(abstracts[node_pair[1]]))\n",
    "    X_test[i,5] = np.dot(embeddings_walks[node_pair[0],:], embeddings_walks[node_pair[1],:])/(np.linalg.norm(embeddings_walks[node_pair[0],:])*np.linalg.norm(embeddings_walks[node_pair[1],:]))\n",
    "    X_test[i,6] = np.dot(embeddings_abstracts[node_pair[0],:], embeddings_abstracts[node_pair[1],:])/((np.linalg.norm(embeddings_abstracts[node_pair[0],:])*np.linalg.norm(embeddings_abstracts[node_pair[1],:])+1))\n",
    "    X_test[i,7] = len(authors_sets[node_pair[0]].intersection(authors_sets[node_pair[1]]))\n",
    "    X_test[i,8] = model_dw.wv.similarity(w1=str(node_pair[0]), w2=str(node_pair[1]))\n",
    "    X_test[i,9] = fuzz.partial_ratio(' '.join(list_authors[node_pair[0]]),' '.join(list_authors[node_pair[1]]))\n",
    "    \n",
    "\n",
    "print('Size of testing matrix:', X_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d90c1a77",
   "metadata": {},
   "source": [
    "print(np.isnan(X_train).sum())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b99a4c0",
   "metadata": {},
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "\n",
    "y_test = np.loadtxt(urllib.request.urlopen('https://resourcesftp.blob.core.windows.net/files/aai/challenge/y_test.txt'), delimiter=',', usecols=1)\n",
    "print('Log loss:', log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0f30782",
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "model_xgb = XGBClassifier()\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred = model_xgb.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "y_test = np.loadtxt(urllib.request.urlopen('https://resourcesftp.blob.core.windows.net/files/aai/challenge/y_test.txt'), delimiter=',', usecols=1)\n",
    "print('Log loss:', log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a50a788f",
   "metadata": {},
   "source": [
    "# MLP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, output_dim)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 256\n",
    "input_dim = 10\n",
    "hidden_dim_1 = 12\n",
    "hidden_dim_2 = 16\n",
    "output_dim = 1\n",
    "model = Net(input_dim, hidden_dim_1, hidden_dim_2, output_dim)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def train(x, y):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "def test(x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "    return output\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    n_train = X_train.shape[0]\n",
    "    loss_all = 0\n",
    "    idx = np.random.permutation(n_train)\n",
    "    for i in range(0, n_train, batch_size):\n",
    "        x = X_train[idx[i:min(i+batch_size,n_train)],:]\n",
    "        x = torch.FloatTensor(x).to(device)\n",
    "        y = y_train[idx[i:min(i+batch_size,n_train)]]\n",
    "        y = torch.FloatTensor(y).to(device)\n",
    "        loss = train(x, y)\n",
    "        loss_all += loss * (min(i+batch_size,n_train)-i)\n",
    "\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print('Epoch: {:03d}, Loss: {:.4f}'.format(epoch, loss_all/n_train))\n",
    "\n",
    "n_test = X_test.shape[0]\n",
    "y_pred = list()\n",
    "for i in range(0, n_train, batch_size):\n",
    "    x = X_test[i:min(i+batch_size,n_test),:]\n",
    "    x = torch.FloatTensor(x).to(device)\n",
    "    output = test(x)\n",
    "    y_pred.append(output.detach().cpu())\n",
    "\n",
    "y_pred = torch.cat(y_pred, dim=0).numpy()\n",
    "y_pred = y_pred.astype(np.float64)\n",
    "\n",
    "y_test = np.loadtxt(urllib.request.urlopen('https://resourcesftp.blob.core.windows.net/files/aai/challenge/y_test.txt'), delimiter=',', usecols=1)\n",
    "print('Log loss:', log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f329e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 7.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-Levenshtein) (49.6.0.post20210108)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp36-cp36m-linux_x86_64.whl size=155937 sha256=950892b84d8d98013ad05b242a70e861e004196396dc7f5c541534fb555ee0c2\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4a/a4/bf/d761b0899395c75fa76d003d607b3869ee47f5035b8afc30a2\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d617b0",
   "metadata": {},
   "source": [
    "## Combined Graph, Deepwalk & text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d1bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined Graph, deepwalk & text\n",
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from random import choice\n",
    "from deepwalk import deepwalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "707ff95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 33225\n",
      "Number of edges: 532655\n"
     ]
    }
   ],
   "source": [
    "# Create a directed graph for in_degrees & out_degrees\n",
    "#G = nx.read_edgelist('edgelist.txt', delimiter=' ', create_using=nx.Graph(), nodetype=int)\n",
    "G = nx.read_weighted_edgelist('edgelist.txt', delimiter=' ', create_using=nx.DiGraph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "edges = list(G.edges())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0919cc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 33225\n",
      "Number of edges: 506748\n"
     ]
    }
   ],
   "source": [
    "#create an undirected graph G_ for the deepwalk embeddings\n",
    "G_ = nx.read_edgelist('edgelist.txt', delimiter=' ', create_using=nx.Graph(), nodetype=int)\n",
    "nodes_ = list(G_.nodes())\n",
    "edges_ = list(G_.edges())\n",
    "n_ = G_.number_of_nodes()\n",
    "m_ = G_.number_of_edges()\n",
    "print('Number of nodes:', n_)\n",
    "print('Number of edges:', m_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fda28a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating walks\n",
      "Training word2vec\n"
     ]
    }
   ],
   "source": [
    "#generate random walks on the undirected graph\n",
    "# Extracts a set of random walks from the network and feeds them to the Skipgram model\n",
    "n_dim = 64\n",
    "n_walks = 5\n",
    "walk_length = 10\n",
    "model = deepwalk(G_, n_walks, walk_length, n_dim) \n",
    "\n",
    "embeddings = np.zeros((n+1, n_dim))\n",
    "for node_ in G_.nodes():\n",
    "    embeddings[node_,:] = model.wv[str(node_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f7aabf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33226"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e637aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the textual content of each domain name\n",
    "text = list()\n",
    "for i in range(33226):\n",
    "    with open('node_information/text/'+str(i)+'.txt', 'r', errors='ignore') as f:\n",
    "        text.append(f.read())\n",
    "\n",
    "# Map text to set of terms\n",
    "text = [set(text[i].split()) for i in range(len(text))]\n",
    "\n",
    "#train\n",
    "X_train = np.zeros((2*m, 10))\n",
    "y_train = np.zeros(2*m)\n",
    "for i,edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    X_train[2*i,0] = G.in_degree(edge[0])\n",
    "    X_train[2*i,1] = G.out_degree(edge[0])\n",
    "    X_train[2*i,2] = G.in_degree(edge[1])\n",
    "    X_train[2*i,3] = G.out_degree(edge[1])\n",
    "    X_train[2*i,4] = len(text[edge[0]])\n",
    "    X_train[2*i,5] = len(text[edge[1]])\n",
    "    X_train[2*i,6] = len(text[edge[0]].intersection(text[edge[1]]))\n",
    "    X_train[2*i,7] = G_.degree(edge[0]) + G_.degree(edge[1])\n",
    "    X_train[2*i,8] = abs(G_.degree(edge[0]) - G_.degree(edge[1]))\n",
    "    X_train[2*i,9] = np.dot(embeddings[edge[0],:], embeddings[edge[1],:])/(np.linalg.norm(embeddings[edge[0],:])*np.linalg.norm(embeddings[edge[1],:]))\n",
    "    y_train[2*i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    X_train[2*i+1,0] = G.in_degree(n1)\n",
    "    X_train[2*i+1,1] = G.out_degree(n1)\n",
    "    X_train[2*i+1,2] = G.in_degree(n2)\n",
    "    X_train[2*i+1,3] = G.out_degree(n2)\n",
    "    X_train[2*i+1,4] = len(text[n1])\n",
    "    X_train[2*i+1,5] = len(text[n2])\n",
    "    X_train[2*i+1,6] = len(text[n1].intersection(text[n2]))\n",
    "    X_train[2*i+1,7] = G_.degree(n1) + G_.degree(n2)\n",
    "    X_train[2*i+1,8] = abs(G_.degree(n1) - G_.degree(n2))\n",
    "    X_train[2*i+1,9] = np.dot(embeddings[n1,:], embeddings[n2,:])/(np.linalg.norm(embeddings[n1,:])*np.linalg.norm(embeddings[n2,:]))\n",
    "    y_train[2*i+1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14318331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))\n",
    "\n",
    "# Create the test matrix. Use the same 7 features as above\n",
    "X_test = np.zeros((len(node_pairs), 10))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = G.in_degree(node_pair[0])\n",
    "    X_test[i,1] = G.out_degree(node_pair[0])\n",
    "    X_test[i,2] = G.in_degree(node_pair[1])\n",
    "    X_test[i,3] = G.out_degree(node_pair[1])\n",
    "    X_test[i,4] = len(text[node_pair[0]])\n",
    "    X_test[i,5] = len(text[node_pair[1]])\n",
    "    X_test[i,6] = len(text[node_pair[0]].intersection(text[node_pair[1]]))\n",
    "    X_test[i,7] = G_.degree(node_pair[0]) + G_.degree(node_pair[1])\n",
    "    X_test[i,8] = abs(G_.degree(node_pair[0]) - G_.degree(node_pair[1]))\n",
    "    X_test[i,9] = np.dot(embeddings[node_pair[0],:], embeddings[node_pair[1],:])/(np.linalg.norm(embeddings[node_pair[0],:])*np.linalg.norm(embeddings[node_pair[1],:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "714cdf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8771249683190808"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83efebdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:25:30] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9697740563779557"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternative models\n",
    "import xgboost as xgb\n",
    "# Use XGBoost to predict if two nodes are linked by an edge\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8ee6891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "y_pred = y_pred[:,1]\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b2e6f",
   "metadata": {},
   "source": [
    "## 0.9697740563779557 Graph, Deepwalk, & text in XGBoost - 0.16427 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3570eaa",
   "metadata": {},
   "source": [
    "## Extra deepwalk features (common terms & euclidian distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ff74d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined Graph, deepwalk & text\n",
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from random import choice\n",
    "from deepwalk import deepwalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b715821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 33225\n",
      "Number of edges: 532655\n"
     ]
    }
   ],
   "source": [
    "# Create a directed graph for in_degrees & out_degrees\n",
    "#G = nx.read_edgelist('edgelist.txt', delimiter=' ', create_using=nx.Graph(), nodetype=int)\n",
    "G = nx.read_weighted_edgelist('edgelist.txt', delimiter=' ', create_using=nx.DiGraph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "edges = list(G.edges())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abba3557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 33225\n",
      "Number of edges: 506748\n"
     ]
    }
   ],
   "source": [
    "#create an undirected graph G_ for the deepwalk embeddings\n",
    "G_ = nx.read_edgelist('edgelist.txt', delimiter=' ', create_using=nx.Graph(), nodetype=int)\n",
    "nodes_ = list(G_.nodes())\n",
    "edges_ = list(G_.edges())\n",
    "n_ = G_.number_of_nodes()\n",
    "m_ = G_.number_of_edges()\n",
    "print('Number of nodes:', n_)\n",
    "print('Number of edges:', m_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed429af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating walks\n",
      "Training word2vec\n"
     ]
    }
   ],
   "source": [
    "#generate random walks on the undirected graph\n",
    "# Extracts a set of random walks from the network and feeds them to the Skipgram model\n",
    "n_dim = 64\n",
    "n_walks = 5\n",
    "walk_length = 10\n",
    "model = deepwalk(G_, n_walks, walk_length, n_dim) \n",
    "\n",
    "embeddings = np.zeros((n+1, n_dim))\n",
    "for node_ in G_.nodes():\n",
    "    embeddings[node_,:] = model.wv[str(node_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a80dc019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the textual content of each domain name\n",
    "text = list()\n",
    "for i in range(33226):\n",
    "    with open('node_information/text/'+str(i)+'.txt', 'r', errors='ignore') as f:\n",
    "        text.append(f.read())\n",
    "\n",
    "# Map text to set of terms\n",
    "text = [set(text[i].split()) for i in range(len(text))]\n",
    "\n",
    "#train\n",
    "X_train = np.zeros((2*m, 11))\n",
    "y_train = np.zeros(2*m)\n",
    "for i,edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    X_train[2*i,0] = G.in_degree(edge[0]) #in degree source node\n",
    "    X_train[2*i,1] = G.out_degree(edge[0]) #out degree esource node\n",
    "    X_train[2*i,2] = G.in_degree(edge[1]) #in degree target node\n",
    "    X_train[2*i,3] = G.out_degree(edge[1]) #out degree target node \n",
    "    X_train[2*i,4] = len(text[edge[0]]) #number of unique terms of the source node's textual content\n",
    "    X_train[2*i,5] = len(text[edge[1]]) #number of unique terms of the target node's textual content\n",
    "    X_train[2*i,6] = len(text[edge[0]].intersection(text[edge[1]])) #number of common terms between the textual content of the two nodes\n",
    "    X_train[2*i,7] = G_.degree(edge[0]) + G_.degree(edge[1]) #sum of degrees of two nodes\n",
    "    X_train[2*i,8] = abs(G_.degree(edge[0]) - G_.degree(edge[1])) #abs diff of degrees of two nodes \n",
    "    X_train[2*i,9] = np.dot(embeddings[edge[0],:], embeddings[edge[1],:])/(np.linalg.norm(embeddings[edge[0],:])*np.linalg.norm(embeddings[edge[1],:])) #cosine similarity of embeddings of two nodes\n",
    "    #X_train[2*i,10] = len(embeddings[edge[0]].intersection(embeddings[edge[1]])) #nb of common terms betweeen embeddings of pages of the two nodes\n",
    "    X_train[2*i,10] = np.linalg.norm(embeddings[edge[0],:]-embeddings[edge[1],:])    #euclidian distance of embeddings of pages\n",
    "    y_train[2*i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = nodes[randint(0, n-1)]\n",
    "    n2 = nodes[randint(0, n-1)]\n",
    "    X_train[2*i+1,0] = G.in_degree(n1) #in degree source node\n",
    "    X_train[2*i+1,1] = G.out_degree(n1)  #out degree esource node\n",
    "    X_train[2*i+1,2] = G.in_degree(n2) #in degree target node\n",
    "    X_train[2*i+1,3] = G.out_degree(n2) #out degree target nod\n",
    "    X_train[2*i+1,4] = len(text[n1])  #number of unique terms of the source node's textual content\n",
    "    X_train[2*i+1,5] = len(text[n2]) #number of unique terms of the target node's textual content\n",
    "    X_train[2*i+1,6] = len(text[n1].intersection(text[n2])) #number of common terms between the textual content of the two nodes\n",
    "    X_train[2*i+1,7] = G_.degree(n1) + G_.degree(n2)  #sum of degrees of two nodes\n",
    "    X_train[2*i+1,8] = abs(G_.degree(n1) - G_.degree(n2)) #abs diff of degrees of two nodes \n",
    "    X_train[2*i+1,9] = np.dot(embeddings[n1,:], embeddings[n2,:])/(np.linalg.norm(embeddings[n1,:])*np.linalg.norm(embeddings[n2,:])) #cosine similarity of embeddings of two nodes\n",
    "    #X_train[2*i+1,10] = len(embeddings[n1].intersection(embeddings[n2])) #nb of common terms betweeen embeddings of pages of the two nodes\n",
    "    X_train[2*i+1,10] = np.linalg.norm(embeddings[n1,:]-embeddings[n2,:]) #euclidian distance of embeddings of pages\n",
    "    y_train[2*i+1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ac822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))\n",
    "\n",
    "# Create the test matrix. Use the same 12 features as above\n",
    "X_test = np.zeros((len(node_pairs), 11))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = G.in_degree(node_pair[0])\n",
    "    X_test[i,1] = G.out_degree(node_pair[0])\n",
    "    X_test[i,2] = G.in_degree(node_pair[1])\n",
    "    X_test[i,3] = G.out_degree(node_pair[1])\n",
    "    X_test[i,4] = len(text[node_pair[0]])\n",
    "    X_test[i,5] = len(text[node_pair[1]])\n",
    "    X_test[i,6] = len(text[node_pair[0]].intersection(text[node_pair[1]]))\n",
    "    X_test[i,7] = G_.degree(node_pair[0]) + G_.degree(node_pair[1])\n",
    "    X_test[i,8] = abs(G_.degree(node_pair[0]) - G_.degree(node_pair[1]))\n",
    "    X_test[i,9] = np.dot(embeddings[node_pair[0],:], embeddings[node_pair[1],:])/(np.linalg.norm(embeddings[node_pair[0],:])*np.linalg.norm(embeddings[node_pair[1],:]))\n",
    "    #X_test[i,10] = len(embeddings[node_pair[0]].intersection(embeddings[node_pair[1]]))\n",
    "    X_test[i,10] = np.linalg.norm(embeddings[node_pair[0],:]-embeddings[node_pair[1],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b57b2be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91099210558429"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662dbd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:10:15] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9704452225173893"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternative models\n",
    "import xgboost as xgb\n",
    "# Use XGBoost to predict if two nodes are linked by an edge\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "405bd37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "y_pred = y_pred[:,1]\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae8b576",
   "metadata": {},
   "source": [
    "## 0.9704452225173893 for xgboost with addition of euclidian distance on deep walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7c5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
